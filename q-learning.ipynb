{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from qiskit import(\n",
    "  QuantumCircuit,\n",
    "  execute,\n",
    "  Aer)\n",
    "import qutip as qt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the target state\n",
    "\n",
    "# Use Aer's qasm_simulator\n",
    "simulator = Aer.get_backend('statevector_simulator')\n",
    "\n",
    "# Create a Quantum Circuit acting on the q register\n",
    "circuit = QuantumCircuit(2, 2)\n",
    "\n",
    "# Add a H gate on qubit 0\n",
    "circuit.h(0)\n",
    "circuit.cx(0,1)\n",
    "circuit.t(0)\n",
    "circuit.h(1)\n",
    "\n",
    "\n",
    "\n",
    "# Draw the circuit\n",
    "circuit.draw()\n",
    "\n",
    "# Execute the circuit on the qasm simulator\n",
    "job = execute(circuit, simulator, shots=10)\n",
    "\n",
    "# Grab results from the job\n",
    "result = job.result()\n",
    "out_state = result.get_statevector()\n",
    "print(out_state)\n",
    "circuit.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class level2():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.simulator = Aer.get_backend('statevector_simulator')\n",
    "        \n",
    "        self.max_circuit_length = 6\n",
    "        self.action_space = 10\n",
    "        self.observation_space = (1+self.action_space)**self.max_circuit_length\n",
    "        \n",
    "        self.target_state = np.array([0.85355339+0.35355339j, 0. +0.j, 0.14644661-0.35355339j,0.+0.j], dtype=np.complex_)\n",
    "        \n",
    "        self.target_state_vector = qt.Qobj(self.target_state)\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        self.current_circuit_length += 1\n",
    "        \n",
    "        if( self.current_circuit_length > self.max_circuit_length ):\n",
    "            done = True\n",
    "            reward = -1\n",
    "            return self.state, reward, done, {'reason':'circuit too long'}\n",
    "\n",
    "        \n",
    "        # Update the state\n",
    "        self.state[self.current_circuit_length-1] = action\n",
    "\n",
    "        # If we get here, then we are still below max_circuit_length\n",
    "        if( action == 0 ):\n",
    "            self.circuit.x(0)\n",
    "        if( action == 1 ):\n",
    "            self.circuit.h(0)\n",
    "        if( action == 2 ):\n",
    "            self.circuit.z(0)\n",
    "        if( action == 3 ):\n",
    "            self.circuit.t(0)\n",
    "        if( action == 4 ):\n",
    "            self.circuit.cx(0,1)  \n",
    "            \n",
    "        if( action == 5 ):\n",
    "            self.circuit.x(1)\n",
    "        if( action == 6 ):\n",
    "            self.circuit.h(1)\n",
    "        if( action == 7 ):\n",
    "            self.circuit.z(1)\n",
    "        if( action == 8 ):\n",
    "            self.circuit.t(1)        \n",
    "        if( action == 9 ):\n",
    "            self.circuit.cx(1,0)\n",
    "            \n",
    "        # Choose simulator backend, etc\n",
    "        # Run the circuit\n",
    "        job = execute(self.circuit, self.simulator, shots=1)\n",
    "        result = job.result()\n",
    "        current_quantum_state = result.get_statevector(0)\n",
    "        \n",
    "        self.tracedistance = qt.metrics.tracedist(qt.Qobj(current_quantum_state), (self.target_state_vector))\n",
    "        \n",
    "        reward = 1-self.tracedistance\n",
    "        \n",
    "        # Problem solved?\n",
    "        if np.linalg.norm(current_quantum_state - self.target_state ) < 1e-5:\n",
    "            done = True\n",
    "#             reward = 1-self.tracedistance\n",
    "            # TODO: Change reward such that shorter circuit gives higher reward\n",
    "#             reward = self.max_circuit_length - self.current_circuit_length\n",
    "        \n",
    "#             return self.state, reward , done, {'reason':'solved!'}\n",
    "        \n",
    "#         if self.tracedistance > 1e-5 and self.tracedistance < 0.25:\n",
    "#             reward = 3\n",
    "#             return self.state, reward, done, {}\n",
    "        \n",
    "#         if self.tracedistance >= 0.25 and self.tracedistance < 0.5:\n",
    "#             reward = 2\n",
    "#             return self.state, reward, done, {}\n",
    "        \n",
    "#         if self.tracedistance >= 0.5 and self.tracedistance < 0.75:\n",
    "#             reward = 1\n",
    "#             return self.state, reward, done, {}        \n",
    "        \n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_circuit_length = 0\n",
    "        self.state = -np.ones(self.max_circuit_length) # [-1 -1 -1 -1 -1]\n",
    "\n",
    "        # Create a Quantum Circuit acting on the q register\n",
    "        self.circuit = QuantumCircuit(2, 2)\n",
    "        return self.state\n",
    "\n",
    "    def encode(self, state):\n",
    "        # Each index can take 5 values (-1, 0, 1, 2, 3)\n",
    "        number = 0\n",
    "        for i,s in enumerate(state):\n",
    "          number += (s+1)*5**i\n",
    "\n",
    "        return int(number)\n",
    "\n",
    "    def render(self):\n",
    "        self.circuit.draw()\n",
    "        return\n",
    "\n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1615296094925,
     "user": {
      "displayName": "Evert van Nieuwenburg",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjNEK4mddxhnYyYu9yyWrbaeJdr48yzsJg4TJwCDFM=s64",
      "userId": "04228112365008071637"
     },
     "user_tz": -60
    },
    "id": "ursrj088A1J0",
    "outputId": "eed1ef58-dce1-4c1a-fb6d-0bcf8c1c5676"
   },
   "outputs": [],
   "source": [
    "env = level2()\n",
    "initial_state = env.reset()\n",
    "\n",
    "env.step(1)\n",
    "env.step(4)\n",
    "env.step(3)\n",
    "env.step(6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34185,
     "status": "ok",
     "timestamp": 1615296157746,
     "user": {
      "displayName": "Evert van Nieuwenburg",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjNEK4mddxhnYyYu9yyWrbaeJdr48yzsJg4TJwCDFM=s64",
      "userId": "04228112365008071637"
     },
     "user_tz": -60
    },
    "id": "J08E0TJA4tEW",
    "outputId": "9678cf7a-b9bb-4db0-e445-4d7d044b313b"
   },
   "outputs": [],
   "source": [
    "env = level2()\n",
    "state = env.reset() # reset environment to a new, random state\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.8\n",
    "gamma = 1\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.9995\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_rewards = []\n",
    "\n",
    "# Q(s, a) = \"Expected total reward to get, if in state 's' I take action 'a'\"\n",
    "\n",
    "q_table = np.zeros([env.observation_space, env.action_space])\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, reward = 0, 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        state_as_number = env.encode(state)\n",
    "\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = np.random.randint(env.action_space) # Explore action space\n",
    "        \n",
    "        else:\n",
    "            action = np.argmax(q_table[state_as_number]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        total_reward += reward\n",
    "                \n",
    "        next_state_as_number = env.encode(next_state)\n",
    "        \n",
    "        old_value = q_table[state_as_number, action]\n",
    "        next_max = np.max(q_table[next_state_as_number])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state_as_number, action] = new_value\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "\n",
    "    all_rewards.append(total_reward)\n",
    "    \n",
    "    epsilon *= epsilon_decay\n",
    "    if( epsilon < 0.01):\n",
    "      epsilon = 0.01\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Episode: {i}\")\n",
    "        print(\"Current exploration rate: %.2f\"%epsilon)\n",
    "        \n",
    "        # Check if it won\n",
    "    if i == 2999:\n",
    "        print(q_table)\n",
    "        \n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(int(len(all_rewards)/100)):\n",
    "#     all_rewards[i*100:(i+1)*100]\n",
    "\n",
    "rewards = np.split(np.array(all_rewards), 100)\n",
    "\n",
    "reward_mean = [ np.mean(r) for r in rewards ]\n",
    "\n",
    "reward_std = [ np.std(r) for r in rewards ]\n",
    "\n",
    "X = np.arange(100)\n",
    "plt.plot(X, reward_mean, '.')\n",
    "plt.errorbar(X, reward_mean,  reward_std, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1615296260063,
     "user": {
      "displayName": "Evert van Nieuwenburg",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjNEK4mddxhnYyYu9yyWrbaeJdr48yzsJg4TJwCDFM=s64",
      "userId": "04228112365008071637"
     },
     "user_tz": -60
    },
    "id": "mxKWXoEwEgTQ",
    "outputId": "8f17de0f-d57c-4c9a-8cee-221ae5125dc8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 1\n",
    "\n",
    "wins = 0\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    state_as_num = env.encode(state)\n",
    "    epochs, reward = 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Starting in state \")\n",
    "    print(state, state_as_num)\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state_as_num])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        state_as_num = env.encode(state)\n",
    "\n",
    "        print(\"Took action: %d\"%action)\n",
    "        print(\"New state and num: \")\n",
    "        print(state, state_as_num)\n",
    "        \n",
    "        epochs += 1\n",
    "\n",
    "    if reward > 0:\n",
    "        wins += 1\n",
    "\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Wins: {wins}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
